{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3,4,5'\n",
    "import torch\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES Count: 0\n",
      "[[3, 8, 4, 9, 1], [6, 11, 10, 4, 5, 7, 2]]\n",
      "[[9, 11, 10, 4, 6, 1], [3, 7, 8, 7, 10, 12, 5, 2]]\n",
      "end!\n",
      "tensor([[ 3,  8,  4,  9,  1,  0,  0,  0],\n",
      "        [ 6, 11, 10,  4,  5,  7,  2,  0]], dtype=torch.int32)\n",
      "tensor([[ 9, 11, 10,  4,  6,  1,  0,  0,  0,  0],\n",
      "        [ 3,  7,  8,  7, 10, 12,  5,  2,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3,4,5'\n",
    "import torch\n",
    "print('CUDA_VISIBLE_DEVICES Count:',torch.cuda.device_count())\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 关于word embedding， 以序列建模为例\n",
    "\n",
    "src = ['I have a hd ,','but it is a big dog .']\n",
    "tgt = ['wo you yi ge hd ,','dan shi ta shi yi zhi gou .']\n",
    "src_len = 8  # for pad and position embedding \n",
    "tgt_len = 10  # for pad and position embedding \n",
    "max_pos_len = 20\n",
    "# word_pad = ['P', 'S', 'E']\n",
    "\n",
    "import numpy as np\n",
    "# 从原始的句子生成字典和序号输入\n",
    "def sentence2input(sentence):\n",
    "    '''\n",
    "    input: sentence\n",
    "    output: sequence, word2idx, idx2word\n",
    "    '''\n",
    "    source_split_words = [s.split(' ') for s in sentence]\n",
    "    word = []\n",
    "    for i in [j.split(' ') for j in sentence]:\n",
    "        word.extend(i)\n",
    "    # generate vocabulary =================\n",
    "    vocab = np.array(word)\n",
    "    vocab = np.unique(vocab)\n",
    "    idx2word = dict(enumerate(vocab,start=1))  # 从1开始因为pad填充0\n",
    "    word2idx = {v: k for k, v in idx2word.items()}\n",
    "    # word2idx = {w:i for i, w in enumerate(vocab)} \n",
    "    sequence = []\n",
    "    for n in source_split_words:\n",
    "        bs = [word2idx[w] for w in n]\n",
    "        sequence.append(bs)\n",
    "    return sequence, word2idx, idx2word\n",
    "\n",
    "\n",
    "def pad(input,max_len,pad_value=0):\n",
    "    '''\n",
    "    input: sentence\n",
    "    output: sequence, word2idx, idx2word\n",
    "    '''\n",
    "    import copy\n",
    "    pad_ = []\n",
    "    for i in input:\n",
    "        # print(i)\n",
    "        ii = copy.deepcopy(i)\n",
    "        if len(i)<max_len:\n",
    "            error_len = max_len - len(i)\n",
    "            for _ in range(error_len):\n",
    "                ii.append(pad_value)\n",
    "        pad_.append(ii[:max_len])\n",
    "    pad_ = torch.IntTensor(pad_)\n",
    "    return pad_\n",
    "\n",
    "src_input, src_vocab_word2idx, enc_vocab_idx2word = sentence2input(src)\n",
    "tgt_input, tgt_vocab_word2idx, tgt_vocab_idx2word = sentence2input(tgt)\n",
    "src_vocab_len = len(src_vocab_word2idx)  # 后面会用到\n",
    "tgt_vocab_len = len(tgt_vocab_word2idx)  # 后面会用到\n",
    "print(src_input)\n",
    "print(tgt_input)\n",
    "print('end!')\n",
    "src_input_T = pad(src_input,src_len,pad_value=0)\n",
    "tgt_input_T = pad(tgt_input,tgt_len,pad_value=0)\n",
    "print(src_input_T)\n",
    "print(tgt_input_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-9.1080e-01,  7.6806e-01,  1.5772e+00,  1.0454e+00, -1.4645e+00,\n",
      "          6.7724e-01, -6.8672e-01, -1.0780e+00],\n",
      "        [ 5.8540e-01, -1.3271e-01, -2.1287e-01,  3.7641e-01, -3.5243e-01,\n",
      "         -8.8952e-01, -2.4938e+00,  9.4424e-01],\n",
      "        [ 7.1382e-01,  5.1468e-01, -1.7282e+00, -7.8384e-04, -7.4555e-02,\n",
      "         -5.9510e-01,  1.6940e+00,  7.2510e-01],\n",
      "        [-1.6171e-01, -1.0535e+00, -1.0095e-01, -2.2332e-01,  8.0977e-01,\n",
      "         -1.4494e-01, -7.9254e-01,  7.3033e-01],\n",
      "        [-1.2048e-01,  2.7927e-01,  8.3582e-01,  4.2372e-01,  5.6566e-01,\n",
      "         -3.7958e-02,  2.2173e-01,  8.5504e-01],\n",
      "        [ 8.9905e-03, -3.9188e-01,  1.3421e+00, -1.0208e+00,  4.8851e-02,\n",
      "         -5.1760e-01,  9.3678e-01,  1.1687e+00],\n",
      "        [-1.0063e+00, -4.0299e-01,  8.0176e-01, -2.5186e+00,  4.9960e-01,\n",
      "          7.9723e-02,  7.5859e-01,  1.1879e+00],\n",
      "        [-8.2110e-01,  3.5953e-01, -6.9386e-01, -1.7914e+00, -7.4943e-01,\n",
      "         -1.1318e+00, -8.3349e-02, -7.0045e-01],\n",
      "        [ 2.0285e+00, -8.5278e-01, -5.4147e-01, -9.4160e-01,  3.9929e-01,\n",
      "          8.8902e-01,  1.5623e-01, -1.5477e+00],\n",
      "        [ 1.4172e+00,  2.6596e+00,  6.1330e-01, -4.7180e-01, -1.6680e-01,\n",
      "          1.9438e+00, -1.4977e+00,  4.3433e-01],\n",
      "        [-1.7847e+00,  1.6079e+00,  1.0299e+00, -8.4517e-01,  1.0547e+00,\n",
      "         -1.6479e+00,  1.6791e-01,  7.6758e-01],\n",
      "        [-3.7619e-01, -3.6890e-01,  8.7430e-01, -6.3164e-01,  6.7812e-01,\n",
      "          4.8789e-01,  3.1080e-01,  1.2624e-01]], requires_grad=True)\n",
      "tensor([[ 3,  8,  4,  9,  1,  0,  0,  0],\n",
      "        [ 6, 11, 10,  4,  5,  7,  2,  0]], dtype=torch.int32)\n",
      "tensor([[[-1.6171e-01, -1.0535e+00, -1.0095e-01, -2.2332e-01,  8.0977e-01,\n",
      "          -1.4494e-01, -7.9254e-01,  7.3033e-01],\n",
      "         [ 2.0285e+00, -8.5278e-01, -5.4147e-01, -9.4160e-01,  3.9929e-01,\n",
      "           8.8902e-01,  1.5623e-01, -1.5477e+00],\n",
      "         [-1.2048e-01,  2.7927e-01,  8.3582e-01,  4.2372e-01,  5.6566e-01,\n",
      "          -3.7958e-02,  2.2173e-01,  8.5504e-01],\n",
      "         [ 1.4172e+00,  2.6596e+00,  6.1330e-01, -4.7180e-01, -1.6680e-01,\n",
      "           1.9438e+00, -1.4977e+00,  4.3433e-01],\n",
      "         [ 5.8540e-01, -1.3271e-01, -2.1287e-01,  3.7641e-01, -3.5243e-01,\n",
      "          -8.8952e-01, -2.4938e+00,  9.4424e-01],\n",
      "         [-9.1080e-01,  7.6806e-01,  1.5772e+00,  1.0454e+00, -1.4645e+00,\n",
      "           6.7724e-01, -6.8672e-01, -1.0780e+00],\n",
      "         [-9.1080e-01,  7.6806e-01,  1.5772e+00,  1.0454e+00, -1.4645e+00,\n",
      "           6.7724e-01, -6.8672e-01, -1.0780e+00],\n",
      "         [-9.1080e-01,  7.6806e-01,  1.5772e+00,  1.0454e+00, -1.4645e+00,\n",
      "           6.7724e-01, -6.8672e-01, -1.0780e+00]],\n",
      "\n",
      "        [[-1.0063e+00, -4.0299e-01,  8.0176e-01, -2.5186e+00,  4.9960e-01,\n",
      "           7.9723e-02,  7.5859e-01,  1.1879e+00],\n",
      "         [-3.7619e-01, -3.6890e-01,  8.7430e-01, -6.3164e-01,  6.7812e-01,\n",
      "           4.8789e-01,  3.1080e-01,  1.2624e-01],\n",
      "         [-1.7847e+00,  1.6079e+00,  1.0299e+00, -8.4517e-01,  1.0547e+00,\n",
      "          -1.6479e+00,  1.6791e-01,  7.6758e-01],\n",
      "         [-1.2048e-01,  2.7927e-01,  8.3582e-01,  4.2372e-01,  5.6566e-01,\n",
      "          -3.7958e-02,  2.2173e-01,  8.5504e-01],\n",
      "         [ 8.9905e-03, -3.9188e-01,  1.3421e+00, -1.0208e+00,  4.8851e-02,\n",
      "          -5.1760e-01,  9.3678e-01,  1.1687e+00],\n",
      "         [-8.2110e-01,  3.5953e-01, -6.9386e-01, -1.7914e+00, -7.4943e-01,\n",
      "          -1.1318e+00, -8.3349e-02, -7.0045e-01],\n",
      "         [ 7.1382e-01,  5.1468e-01, -1.7282e+00, -7.8384e-04, -7.4555e-02,\n",
      "          -5.9510e-01,  1.6940e+00,  7.2510e-01],\n",
      "         [-9.1080e-01,  7.6806e-01,  1.5772e+00,  1.0454e+00, -1.4645e+00,\n",
      "           6.7724e-01, -6.8672e-01, -1.0780e+00]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 构造word embedding\n",
    "model_dim = 8 # 512\n",
    "\n",
    "src_embedding_table = nn.Embedding(src_vocab_len+1,model_dim)  # 初始化一个embedding类，shape：num_embeddings: int, embedding_dim: int\n",
    "tgt_embedding_table = nn.Embedding(tgt_vocab_len+1,model_dim)  # 调用的是nn.Embedding类的forword方法，直接调用类后面一个括号就是调用该类中的forward方法\n",
    "print(src_embedding_table.weight)\n",
    "\n",
    "print(src_input_T)\n",
    "src_embedding = src_embedding_table(src_input_T)\n",
    "tgt_embedding = tgt_embedding_table(tgt_input_T)\n",
    "print(src_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造position embeddings\n",
    "# src_len = 8 是input长度\n",
    "# 实例化nn.Embedding类\n",
    "# src_pos_embedding_table = nn.Embedding(src_len+1,model_dim)  # 初始化一个embedding类，shape：num_embeddings: int, embedding_dim: int\n",
    "# tgt_pos_embedding_table = nn.Embedding(tgt_len+1,model_dim)  # 调用的是nn.Embedding类的forword方法，直接调用类后面一个括号就是调用该类中的forward方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造position embeddings\n",
    "$$ PE(pos, 2i) = sin(pos/10000^{2i/d_model})  $$\n",
    "$$ PE(pos, 2i+1) = cos(pos/1000^{2i/d_model})  $$\n",
    "\n",
    " where $pos$ is the position and $i$ is the dimension. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]], dtype=torch.int32)\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 8]), torch.Size([2, 10, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造position embeddings\n",
    "# src_len = 8 是input长度\n",
    "\n",
    "# 构造全长 position embedding table\n",
    "# max_pos_len = 20\n",
    "pos_mat = torch.arange(max_pos_len).reshape(-1,1)  # tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])\n",
    "i_mat = torch.pow(1000, torch.arange(0,model_dim,2).reshape(1,-1)/model_dim)  # tensor([[  1.0000,   5.6234,  31.6228, 177.8279]])\n",
    "pos_emb_table = torch.zeros(max_pos_len,model_dim)\n",
    "pos_emb_table[:,0::2] = torch.sin(pos_mat/i_mat)\n",
    "pos_emb_table[:,1::2] = torch.cos(pos_mat/i_mat)\n",
    "pos_embedding = nn.Embedding(max_pos_len,model_dim)\n",
    "# pos_embedding.weight  # nn.Embedding.weight随机初始化方式是标准正态分布，即均值μ=0，方差σ=1的正态分布。\n",
    "pos_embedding.weight = nn.Parameter(pos_emb_table,requires_grad=False)  # 这里是修改nn.Embedding类的初始化权重.weight，改为计算出的pos_emb_table\n",
    "#  torch.nn.Parameter是继承自torch.Tensor的子类，其主要作用是作为nn.Module中的可训练参数使用。它与torch.Tensor的区别就是nn.Parameter会自动被认为是module的可训练参数，即加入到parameter()这个迭代器中去；而module中非nn.Parameter()的普通tensor是不在parameter中的。\n",
    "\n",
    "# 获取 position count\n",
    "src_pos = [list(range(src_len)) for _ in src]  # 遍历样本src，src_len=8\n",
    "src_pos = torch.IntTensor(src_pos)\n",
    "print(src_pos)\n",
    "\n",
    "tgt_pos = [list(range(tgt_len)) for _ in src]  # 遍历样本src，tgt_len=10\n",
    "tgt_pos = torch.IntTensor(tgt_pos)\n",
    "print(tgt_pos)\n",
    "\n",
    "src_pos_embedding = pos_embedding(src_pos)  # src 和tgt 输入到一个全长的  position embedding table中\n",
    "tgt_pos_embedding = pos_embedding(tgt_pos)\n",
    "src_pos_embedding.size(), tgt_pos_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造encode 的 self attention mask\n",
    "# mask shape [batchsize, src_len,model_dim]\n",
    "# mask shape [batchsize, tgt_len,model_dim]\n",
    "src_real_len = [len(i.split(' ')) for i in src]\n",
    "valid_encoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.zeros(L,dtype=torch.int32) ,pad=[0,src_len-L],value=1),0) for L in src_real_len],0)\n",
    "print(valid_encoder_pos) # 这是使用unsqueeze的方法 把1维变2维\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos,2) #需要再次unsqueeze 把2维变3维，这样才能使用bmm函数，保留batch size\n",
    "# print(valid_encoder_pos) \n",
    "valid_encoder_pos_metr = (torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1,2)))  # 模拟QK.T相乘\n",
    "mask_encode_self_attention = valid_encoder_pos_metr.to(torch.bool)\n",
    "mask_encode_self_attention\n",
    "\n",
    "# 下面是reshape的方法\n",
    "# src_real_len = [len(i.split(' ')) for i in src]\n",
    "# valid_encoder_pos = torch.cat([F.pad(torch.zeros(L,dtype=torch.int32) ,pad=[0,src_len-L],value=1) for L in src_real_len],0)\n",
    "# valid_encoder_pos = valid_encoder_pos.reshape(len(src),-1)  # 这是使用reshpe的方法\n",
    "# print(valid_encoder_pos)\n",
    "# mask_encode_self_attention = valid_encoder_pos.to(torch.bool)\n",
    "# mask_encode_self_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False, False],\n",
       "         [False, False, False, False, False, False, False,  True]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造encode 的 self-ttention mask\n",
    "# mask shape [batchsize, src_len,model_dim]\n",
    "# mask shape [batchsize, tgt_len,model_dim]\n",
    "src_real_len = [len(i.split(' ')) for i in src]\n",
    "valid_encoder_pos = torch.cat([torch.unsqueeze(F.pad(torch.ones(L,dtype=torch.int32) ,pad=[0,src_len-L],value=0),0) for L in src_real_len],0)\n",
    "print(valid_encoder_pos) # 这是使用unsqueeze的方法 把1维变2维\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos,2) #需要再次unsqueeze 把2维变3维，这样才能使用bmm函数，保留batch size\n",
    "# print(valid_encoder_pos) \n",
    "\n",
    "\n",
    "invalid_encoder_pos = 1 - valid_encoder_pos\n",
    "\n",
    "\n",
    "\n",
    "invalid_encoder_pos_matrix = (torch.bmm(invalid_encoder_pos, invalid_encoder_pos.transpose(1,2)))  # 模拟QK.T相乘\n",
    "mask_encode_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
    "mask_encode_self_attention\n",
    "\n",
    "# 下面是reshape的方法\n",
    "# src_real_len = [len(i.split(' ')) for i in src]\n",
    "# valid_encoder_pos = torch.cat([F.pad(torch.zeros(L,dtype=torch.int32) ,pad=[0,src_len-L],value=1) for L in src_real_len],0)\n",
    "# valid_encoder_pos = valid_encoder_pos.reshape(len(src),-1)  # 这是使用reshpe的方法\n",
    "# print(valid_encoder_pos)\n",
    "# mask_encode_self_attention = valid_encoder_pos.to(torch.bool)\n",
    "# mask_encode_self_attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1583, 0.0269, 0.0217, 0.1735, 0.1535, 0.0742, 0.2146, 0.1774],\n",
       "         [0.0581, 0.0064, 0.1319, 0.1381, 0.0196, 0.0132, 0.0164, 0.6162],\n",
       "         [0.0669, 0.0729, 0.0682, 0.0754, 0.0530, 0.2847, 0.1915, 0.1876],\n",
       "         [0.0220, 0.2650, 0.0197, 0.0444, 0.1443, 0.4586, 0.0097, 0.0363],\n",
       "         [0.2126, 0.0731, 0.1503, 0.0604, 0.0958, 0.0727, 0.0306, 0.3045],\n",
       "         [0.2245, 0.1289, 0.2679, 0.1955, 0.1831, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1057, 0.5387, 0.0365, 0.2916, 0.0275, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0085, 0.8356, 0.0229, 0.0189, 0.1141, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0599, 0.0717, 0.0885, 0.1039, 0.3555, 0.0098, 0.2548, 0.0558],\n",
       "         [0.0212, 0.0192, 0.0344, 0.4755, 0.0918, 0.0125, 0.0440, 0.3014],\n",
       "         [0.0125, 0.1660, 0.0209, 0.0590, 0.0802, 0.3120, 0.1218, 0.2274],\n",
       "         [0.2641, 0.0281, 0.1057, 0.0245, 0.2033, 0.0554, 0.2343, 0.0847],\n",
       "         [0.0961, 0.0202, 0.4509, 0.0865, 0.0368, 0.0429, 0.0759, 0.1908],\n",
       "         [0.1286, 0.1853, 0.0918, 0.0613, 0.0998, 0.0851, 0.2801, 0.0680],\n",
       "         [0.0418, 0.0614, 0.0135, 0.0553, 0.0096, 0.0116, 0.4948, 0.3119],\n",
       "         [0.1463, 0.0193, 0.0073, 0.3528, 0.1425, 0.1710, 0.1607, 0.0000]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask 实施 测试\n",
    "\n",
    "score = torch.randn(len(src),src_len,src_len)\n",
    "score\n",
    "masked_score = score.masked_fill(mask_encode_self_attention, -np.inf)\n",
    "masked_score\n",
    "F.softmax(masked_score,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 1])\n",
      "torch.Size([2, 10, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [False, False, False, False, False, False, False,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构造 intra attention mask \n",
    "src_real_len = [len(i.split(' ')) for i in src]\n",
    "tgt_real_len = [len(i.split(' ')) for i in tgt]\n",
    "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L,dtype=torch.int32) ,pad=[0,src_len-L],value=0),0) for L in src_real_len],0),2)\n",
    "print(valid_encoder_pos.size())\n",
    "valid_decoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L,dtype=torch.int32) ,pad=[0,tgt_len-L],value=0),0) for L in tgt_real_len],0),2)\n",
    "print(valid_decoder_pos.size())\n",
    "valid_cross_pos_matrix = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(1,2))  #注意这里 tgt-decode是Q，src-encoed是K，计算公式是Q和K转置矩阵乘\n",
    "valid_cross_pos_matrix.size()\n",
    "invalid_cross_pos_matrix = 1 - valid_cross_pos_matrix\n",
    "mask_cross_self_attention = invalid_cross_pos_matrix.to(torch.bool)\n",
    "mask_cross_self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# 构造 decode attention mask -- 有点不一样\n",
    "# 构造下三角矩阵 为什么？ 因为decoder的输入要把后面的数据mask掉 每次输入的数据向后移一位 即看到的范围越来越大\n",
    "m_ = [torch.unsqueeze(F.pad(torch.tril(torch.ones((L,L))),(0,(tgt_len-L),0,(tgt_len-L))),0)  for L in tgt_real_len]\n",
    "print(m_[0].size())\n",
    "valide_decoder_tri_matrix = torch.cat(m_)\n",
    "invalide_decoder_tri_matrix = 1 - valide_decoder_tri_matrix\n",
    "invalide_decoder_tri_matrix = invalide_decoder_tri_matrix.to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
       "\n",
       "        [[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "         [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "         [False, False, False, False, False, False, False, False,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invalide_decoder_tri_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1985, 0.8015, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0932, 0.6171, 0.2897, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0899, 0.7133, 0.1502, 0.0467, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0526, 0.1542, 0.7454, 0.0334, 0.0144, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0573, 0.5793, 0.0637, 0.0693, 0.1934, 0.0370, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask 实施 测试\n",
    "\n",
    "score = torch.randn(len(src),tgt_len,tgt_len)\n",
    "score\n",
    "masked_score = score.masked_fill(invalide_decoder_tri_matrix, -1e9)\n",
    "# print(masked_score)\n",
    "F.softmax(masked_score,-1)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造scaled self attention 函数\n",
    "def scale_dot_product_attention(Q,K,V,attention_mask):  # Q K 是 batch的， 同时，也是batch * multi-head的\n",
    "    #  shape of Q K V : [batch_size,num_head,src_len,model_dim/num_head]\n",
    "    score = torch.bmm(Q, K.transpose(-1,-2))/torch.sqrt(model_dim)\n",
    "    masked_score = score.masked_fill(attention_mask, -1e9)\n",
    "    prob = F.softmax(masked_score,-1)\n",
    "    context = torch.bmm(prob,V)\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = nn.Transformer(nhead=2, num_encoder_layers=6)\n",
    "src_ = torch.rand((10,32,512))\n",
    "tgt_ = torch.rand((20,32,512))\n",
    "out = transformer(src_,tgt_)\n",
    "out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4 大类\n",
    "# TransformerEncoder\n",
    "#     TransformerEncoderLayer\n",
    "\n",
    "# TransformerDecoder\n",
    "#     TransformerDe  coderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1777, 0.5750, 0.0000],\n",
       "        [1.9023, 1.1583, 1.7151]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformer Masked loss\n",
    "# 使用机器翻译任务 模拟\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# 两个句子，每个句子3个单词，那就是一共6个单词\n",
    "Logits = torch.randn(2,3,4) # bs=2,seqlen=3,vocab_size=class=4 \n",
    "Logits = Logits.transpose(1,2)  # 在torch 官方crossentropy调用的时候需要的维度是bs，C，d 没有经过softmax之前\n",
    "lable = torch.randint(0,4,(2,3))\n",
    "Logits.size(),lable.size()\n",
    "# 这里的交叉熵是6个单词的平均的交叉熵\n",
    "F.cross_entropy(Logits,lable) \n",
    "# 返回每一个单词的交叉熵\n",
    "F.cross_entropy(Logits,lable,reduction='none') \n",
    "# 构造mask len 即假设tgt的有效长度只有2，有1位是padding的，那就需要把这一位mask掉再进行loss掉统计\n",
    "tgt_len = torch.Tensor([2,3]).to(torch.int32) # 假设两个样本长度为2，3\n",
    "\n",
    "valid_mask = torch.cat([torch.unsqueeze(F.pad(torch.ones(l),(0,max(tgt_len)-l)),0) for l in tgt_len])\n",
    "F.cross_entropy(Logits,lable,reduction='none') * valid_mask\n",
    "\n",
    "# 或者\n",
    "lable[0,2] = -100\n",
    "F.cross_entropy(Logits,lable,reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.8386, -1.2391,  1.2173],\n",
       "          [-0.1758,  0.0315,  2.0426],\n",
       "          [-1.1536,  0.6510, -0.7551],\n",
       "          [-2.0580, -1.7816, -0.6529]],\n",
       " \n",
       "         [[-0.1527,  0.3020, -0.4522],\n",
       "          [-0.2054,  0.5440,  0.6402],\n",
       "          [ 0.0865, -0.5973,  0.3185],\n",
       "          [ 1.5138,  0.6209,  1.3174]]]),\n",
       " tensor([[   2,    2, -100],\n",
       "         [   2,    1,    2]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logits,lable\n",
    "# tensor([[[ 1.5035,  1.1617,  0.8024,  0.4093],\n",
    "#          [ 0.0355, -0.5075, -1.2482, -0.7190],\n",
    "#          [ 0.5320,  1.7259,  0.4466,  0.6156]],\n",
    "\n",
    "# tensor([[3, 1, 1],\n",
    "#          [2, 2, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2, -100],\n",
       "        [   2,    1,    2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "016fd9bb66e6ad19daf5b878b9ae392875dbee4b6103c0b67cb7b40f3b5f6cf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
